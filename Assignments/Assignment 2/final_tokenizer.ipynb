{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Playground Code to put in Assignment Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Tokenizing and Playing with the Text\n",
    "import nltk\n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For Web scraping\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solving SSL Certificate Issue\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_web_read(url):\n",
    "    \"\"\"\n",
    "    This function takes the url as input and outputs the tokens present \n",
    "    on the webpage with the stopwords removed from the token list \n",
    "    TODO: Fiund out whether it is okay to use the stopwords already present in the nltk\n",
    "    library, or do we have to sue the stopwpords as mentioned/linked_to in the Assignment Description.\n",
    "    Ans: Its okay.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the raw html scripting of the url \n",
    "    response = request.urlopen(url)\n",
    "    html = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # Get the Text from the HTML\n",
    "    raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "\n",
    "    # Get All the Tokens, including puncutation and symbols\n",
    "    all_tokens = word_tokenize(raw)\n",
    "\n",
    "    # Get only the alphanumeric words and get rid of all the tokens \n",
    "    # that are stopword, punctuation marks and symbols\n",
    "    only_words = [w for w in all_tokens if w.isalnum() and w not in stopwords.words('english') and len(w) > 3]\n",
    "\n",
    "    return only_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# most common 50 in the token list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Script to find out the 50 most common tokens in all the urls scraped. <br>\n",
    "Approach:<br>\n",
    "Get the `total_token` list from scraper.py, which is the mega collection of all the tokens present in all the urls. find out the 50 most frequent tokens from this list and also return the frequency of each of those tokens (i.e. return a dictionary with token as key and its total frequency as the value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_50_most_common(tokens):\n",
    "    # Here 'tokens' is the mega collection of the tokens in all the urls scraper.py goes through\n",
    "    token_dict = {}\n",
    "    for token in tokens:\n",
    "        if token in token_dict:\n",
    "            token_dict[token] += 1\n",
    "        else:\n",
    "            token_dict[token] = 1\n",
    "            \n",
    "    return token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokens(token_dict):\n",
    "        '''\n",
    "        Finally, write a method that prints out the word frequency count onto the screen. \n",
    "        The print out should be ordered by decreasing frequency (so, the highest frequency \n",
    "        words first).\n",
    "        Input: token_dict (Dictionary of Tokens)\n",
    "        Output: prints th tokens in descending order of their frequencies \n",
    "        Explanation: Takes the token_dict dictionary as input from the driver function.\n",
    "        This dictionary already has the frequencies of the unique tokens calculated stored in it\n",
    "        as a result of the opeations onthe tokens list in the computeWordFrequencies() function.\n",
    "        \n",
    "        '''\n",
    "\n",
    "        sorted_token_list = []\n",
    "        # sorted_token_dict = dict(sorted(token_dict.items(), key=operator.itemgetter(1),reverse=True))\n",
    "\n",
    "        print('The tokens with their frequencies in sorted order (descending) are:')\n",
    "        count = 0\n",
    "        for k, v in (sorted(token_dict.items(), key=lambda item: item[1], reverse = True)):\n",
    "            if count < 50:\n",
    "                print('{} = {}'.format(k, v))\n",
    "                count += 1\n",
    "            else:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens with their frequencies in sorted order (descending) are:\n",
      "NLTK = 19\n",
      "Python = 7\n",
      "Natural = 4\n",
      "Language = 4\n",
      "text = 3\n",
      "tokens = 3\n",
      "tagged = 3\n",
      "entities = 3\n",
      "Documentation = 2\n",
      "Installing = 2\n",
      "programs = 2\n",
      "work = 2\n",
      "language = 2\n",
      "provides = 2\n",
      "corpora = 2\n",
      "processing = 2\n",
      "libraries = 2\n",
      "discussion = 2\n",
      "programming = 2\n",
      "fundamentals = 2\n",
      "computational = 2\n",
      "linguistics = 2\n",
      "available = 2\n",
      "source = 2\n",
      "working = 2\n",
      "Processing = 2\n",
      "version = 2\n",
      "book = 2\n",
      "import = 2\n",
      "sentence = 2\n",
      "Tree = 2\n",
      "2022 = 2\n",
      "Toolkit = 1\n",
      "Reference = 1\n",
      "Example = 1\n",
      "Usage = 1\n",
      "Module = 1\n",
      "Index = 1\n",
      "Wiki = 1\n",
      "Open = 1\n",
      "Issues = 1\n",
      "GitHub = 1\n",
      "Installation = 1\n",
      "Data = 1\n",
      "More = 1\n",
      "Release = 1\n",
      "Notes = 1\n",
      "Contributing = 1\n",
      "Team = 1\n",
      "leading = 1\n"
     ]
    }
   ],
   "source": [
    "# Code to test and get output during development:\n",
    "url = \"http://www.nltk.org\"\n",
    "print_tokens(find_50_most_common(nltk_web_read(url)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
