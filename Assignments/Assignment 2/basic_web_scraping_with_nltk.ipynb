{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://jmausolf.github.io/courses/cfss/webdata_scrape_py_b.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-liminary steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elementary Web Scraping usng NLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/gunthercox/ChatterBot/issues/930#issuecomment-322111087\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *\n",
    "# from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "# Import Web Scraping Modules\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Command al Matplotlib Graphs to Appear\n",
    "# in-line in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_web_read(url, _raw = 0, words = 1):\n",
    "    '''\n",
    "    This function returns the text of the website\n",
    "    for a given URL\n",
    "    OPTIONS:\n",
    "    - _raw = option to return text from HTML\n",
    "            - 0 = no\n",
    "            - 1 = yes, return raw text\n",
    "    - words = option to return word tokens from HTML\n",
    "            - 1 = return all words (default)\n",
    "            - 2 return only alphanumeric words\n",
    "    '''\n",
    "\n",
    "    # Import modules\n",
    "    from urllib import request\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    response = request.urlopen(url)\n",
    "    html = response.read().decode(\"utf-8\")\n",
    "\n",
    "    # Get Text from HTML\n",
    "    raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "    raw[:200]\n",
    "\n",
    "    # Options\n",
    "\n",
    "    # Raw Text Option\n",
    "    if _raw == 0:\n",
    "        pass\n",
    "    else:\n",
    "        print(raw[:200])\n",
    "        # return raw\n",
    "    \n",
    "    # Get Tokens\n",
    "    tokens = word_tokenize(raw)\n",
    "\n",
    "    # Word Options\n",
    "    if words == 1:\n",
    "        print(tokens[:200])\n",
    "    elif words == 2:\n",
    "        words = [w for w in tokens if w.isalnum()]\n",
    "\n",
    "        print(words[:200])\n",
    "        # return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK :: Natural Language Toolkit\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK\n",
      "\n",
      "\n",
      "\n",
      "Documentation\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "NLTK Documentation\n",
      "\n",
      "API Reference\n",
      "Example Usage\n",
      "Module Index\n",
      "Wiki\n",
      "FAQ\n",
      "Open Issues\n",
      "NLTK on GitHub\n",
      "\n",
      "Installation\n",
      "\n",
      "['NLTK', ':', ':', 'Natural', 'Language', 'Toolkit', 'NLTK', 'Documentation', 'NLTK', 'Documentation', 'API', 'Reference', 'Example', 'Usage', 'Module', 'Index', 'Wiki', 'FAQ', 'Open', 'Issues', 'NLTK', 'on', 'GitHub', 'Installation', 'Installing', 'NLTK', 'Installing', 'NLTK', 'Data', 'More', 'Release', 'Notes', 'Contributing', 'to', 'NLTK', 'NLTK', 'Team', 'Natural', 'Language', 'Toolkit¶', 'NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', ',', 'tokenization', ',', 'stemming', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.', 'Thanks', 'to', 'a', 'hands-on', 'guide', 'introducing', 'programming', 'fundamentals', 'alongside', 'topics', 'in', 'computational', 'linguistics', ',', 'plus', 'comprehensive', 'API', 'documentation', ',', 'NLTK', 'is', 'suitable', 'for', 'linguists', ',', 'engineers', ',', 'students', ',', 'educators', ',', 'researchers', ',', 'and', 'industry', 'users', 'alike', '.', 'NLTK', 'is', 'available', 'for', 'Windows', ',', 'Mac', 'OS', 'X', ',', 'and', 'Linux', '.', 'Best', 'of', 'all', ',', 'NLTK', 'is', 'a', 'free', ',', 'open', 'source', ',', 'community-driven', 'project', '.', 'NLTK', 'has', 'been', 'called', '“', 'a', 'wonderful', 'tool', 'for', 'teaching', ',', 'and', 'working', 'in', ',', 'computational', 'linguistics', 'using', 'Python', ',', '”', 'and', '“', 'an', 'amazing', 'library', 'to', 'play']\n"
     ]
    }
   ],
   "source": [
    "# Get all Raw Content\n",
    "url = \"http://www.nltk.org\"\n",
    "nltk_web_read(url, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/samyakjhaveri/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'Natural', 'Language', 'Toolkit', 'NLTK', 'Documentation', 'NLTK', 'Documentation', 'API', 'Reference', 'Example', 'Usage', 'Module', 'Index', 'Wiki', 'FAQ', 'Open', 'Issues', 'NLTK', 'on', 'GitHub', 'Installation', 'Installing', 'NLTK', 'Installing', 'NLTK', 'Data', 'More', 'Release', 'Notes', 'Contributing', 'to', 'NLTK', 'NLTK', 'Team', 'Natural', 'Language', 'NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', 'It', 'provides', 'interfaces', 'to', 'over', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'classification', 'tokenization', 'stemming', 'tagging', 'parsing', 'and', 'semantic', 'reasoning', 'wrappers', 'for', 'NLP', 'libraries', 'and', 'an', 'active', 'discussion', 'forum', 'Thanks', 'to', 'a', 'guide', 'introducing', 'programming', 'fundamentals', 'alongside', 'topics', 'in', 'computational', 'linguistics', 'plus', 'comprehensive', 'API', 'documentation', 'NLTK', 'is', 'suitable', 'for', 'linguists', 'engineers', 'students', 'educators', 'researchers', 'and', 'industry', 'users', 'alike', 'NLTK', 'is', 'available', 'for', 'Windows', 'Mac', 'OS', 'X', 'and', 'Linux', 'Best', 'of', 'all', 'NLTK', 'is', 'a', 'free', 'open', 'source', 'project', 'NLTK', 'has', 'been', 'called', 'a', 'wonderful', 'tool', 'for', 'teaching', 'and', 'working', 'in', 'computational', 'linguistics', 'using', 'Python', 'and', 'an', 'amazing', 'library', 'to', 'play', 'with', 'natural', 'Natural', 'Language', 'Processing', 'with', 'Python', 'provides', 'a', 'practical', 'introduction', 'to', 'programming', 'for', 'language', 'processing', 'Written', 'by', 'the', 'creators', 'of', 'NLTK', 'it', 'guides', 'the', 'reader', 'through', 'the', 'fundamentals', 'of', 'writing', 'Python', 'programs', 'working', 'with', 'corpora', 'categorizing', 'text']\n"
     ]
    }
   ],
   "source": [
    "# Get ONLY Raw Text\n",
    "nltk_web_read(url, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diving Into BeautiulSoup with HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a BeautifulSoup Function\n",
    "def get_website_text(url, div_class = 0, _return =  0):\n",
    "    \"\"\"\n",
    "    -----------------------------------------------------\n",
    "    This function returns the text of a website for a\n",
    "    given URL using Beautiful Soup. \n",
    "    \n",
    "    The URL must be specified.\n",
    "    \n",
    "    If you do not know the HTML format but would like to \n",
    "    try parsing the URL, run as is. The parser looks for \n",
    "    the \"div\" class. However, depending on the webpage,\n",
    "    you may need to first inspect the HTML and specify\n",
    "    a \"div class=<input>\", where \"<input>\" could\n",
    "    be any number of unique strings specific to the \n",
    "    website.\n",
    "    \n",
    "    After finding the content tag, this function returns\n",
    "    text in the paragraph <p> tag.\n",
    "    \n",
    "    -----------------------------------------------------\n",
    "    OPTIONS\n",
    "    -----------------------------------------------------\n",
    "    - div_class = a specified class of the <div> tag\n",
    "        - 0 (default)\n",
    "            - looks for any div tags. Works on some \n",
    "            but not all websites.\n",
    "        - Any string\n",
    "            - looks for that string as a div class\n",
    "        \n",
    "        Example:\n",
    "        \n",
    "        get_website_text(url, \"content-wrapper\")\n",
    "        \n",
    "        This input looks for the tag\n",
    "        \n",
    "        <div class=\"content-wrapper\">. \n",
    "        \n",
    "    -----------------------------------------------------\n",
    "    - _return = option to return text for use in another\n",
    "                function. \n",
    "            - 0 = do not return, print instead (default)\n",
    "            - 1 = return text\n",
    "    -----------------------------------------------------\n",
    "    \"\"\"\n",
    "\n",
    "    # Import modules\n",
    "    from urllib import request\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "    # Get HTML from URL\n",
    "    response = request.urlopen(url)\n",
    "    html = response.read().decode('utf-8')\n",
    "    \n",
    "    # Get Soup for BeautifulSoup\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Class Option (Default = 0)\n",
    "\n",
    "    # Define Content\n",
    "\n",
    "    # Loof for any Div Tag\n",
    "    if div_class == 0:\n",
    "        pass\n",
    "        content = soup.find(\"div\")\n",
    "\n",
    "        # parser content error message\n",
    "        if len(str(content)) < 1000:\n",
    "            print(\"Your request may not be returning the desired results.\", '\\n' \\\n",
    "                    \"Consider inspecting the webpage and trying a different div tag\", '\\n')\n",
    "            print(\"CURRENT RESULTS:\", '\\n', content)\n",
    "        else:\n",
    "            pass\n",
    "    #Look for Specific Div Tag\n",
    "    else:\n",
    "        try:\n",
    "            content = soup.find(\"div\", {\"class\":str(div_class)})\n",
    "            \n",
    "            #Parser Content Error Message\n",
    "            if len(str(content)) < 1000:\n",
    "                print (\"Your request may not be returning the desired results.\", '\\n' \\\n",
    "                          \"Consider inspecting the webpage and trying a different div tag\", '\\n')\n",
    "                print (\"CURRENT RESULTS:\", '\\n', content)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        #Print Error Message For Failure\n",
    "        except:\n",
    "            print (\"Error: Please check your div class='input'.\", \\\n",
    "                   \"A valid 'input' must be specified\")\n",
    "            return\n",
    "    \n",
    "    # Get Paragraph Body\n",
    "    paragraph = [\"\".join(x.findAll(text = True)) for x in content.findAll(\"p\")]\n",
    "    paragraph_body = \"\\n\\n%s\" % (\"\\n\\n\".join(paragraph))\n",
    "\n",
    "    # Return Function Option\n",
    "    if _return == 1:\n",
    "        return paragraph_body\n",
    "    else:\n",
    "        print(paragraph_body)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "NLTK Documentation\n",
      "\n",
      "Installation\n",
      "\n",
      "More\n",
      "\n",
      "NLTK is a leading platform for building Python programs to work with human language data.\n",
      "It provides easy-to-use interfaces to over 50 corpora and lexical\n",
      "resources such as WordNet,\n",
      "along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,\n",
      "wrappers for industrial-strength NLP libraries,\n",
      "and an active discussion forum.\n",
      "\n",
      "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation,\n",
      "NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike.\n",
      "NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open source, community-driven project.\n",
      "\n",
      "NLTK has been called “a wonderful tool for teaching, and working in, computational linguistics using Python,”\n",
      "and “an amazing library to play with natural language.”\n",
      "\n",
      "Natural Language Processing with Python provides a practical\n",
      "introduction to programming for language processing.\n",
      "Written by the creators of NLTK, it guides the reader through the fundamentals\n",
      "of writing Python programs, working with corpora, categorizing text, analyzing linguistic structure,\n",
      "and more.\n",
      "The online version of the book has been been updated for Python 3 and NLTK 3.\n",
      "(The original Python 2 version is still available at https://www.nltk.org/book_1ed.)\n",
      "\n",
      "Tokenize and tag some text:\n",
      "\n",
      "Identify named entities:\n",
      "\n",
      "Display a parse tree:\n",
      "\n",
      "NB. If you publish work that uses NLTK, please cite the NLTK book as\n",
      "follows:\n",
      "\n",
      "Bird, Steven, Edward Loper and Ewan Klein (2009), Natural Language Processing with Python.  O’Reilly Media Inc.\n",
      "\n",
      "Sign up for release announcements\n",
      "\n",
      "Join in the discussion\n"
     ]
    }
   ],
   "source": [
    "# Example NLTK Website\n",
    "url = \"http://www.nltk.org\"\n",
    "get_website_text(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Atlantic Online\n",
    "url = \"http://www.theatlantic.com/politics/archive/2016/02/berniebro-revisited/460212/\"\n",
    "text = get_website_text(url, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I coined the term—now I’ve come back to fix what I started.\n",
      "\n",
      "O reader, hear my plea: I am the victim of semantic drift.\n",
      "\n",
      "Four months ago, I coined the term “Berniebro” to describe a phenomenon I saw on Facebook: Men, mostly my age, mostly of my background, mostly with my political beliefs, were hectoring their friends about how great Bernie was even when their friends wanted to do something else, like talk about the NBA.\n",
      "\n",
      "In the post, I tried to gently suggest that maybe there were other ways to advance Sanders’s beliefs, many of which I share. I hinted, too, that I was not talking about every Sanders supporter. I did this subtly, by writing: “The Berniebro is not every Sanders supporter.”\n",
      "\n",
      "Then, 28,000 people shared the story on Facebook. The Berniebro was alive! Immediately, I started getting emails: Why did I hate progressivism? Why did I joke about politics? And how dare I generalize about every Bernie Sanders supporter?\n",
      "\n",
      "But the worst was yet to come. For now that the Berniebro \n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\tEmbassy of Israel\n",
      "\n",
      "\n",
      "\tWashington, D.C.\n",
      "\n",
      "\n",
      "\t6:54 P.M. EST\n",
      "\n",
      "\n",
      "\t     THE PRESIDENT:  Thank you.  Thank you.  (Applause.)  Good evening.  Erev Tov. \n",
      "\n",
      "\n",
      "\t     The Talmud teaches that if a person destroys one life, it is as if they’ve destroyed an entire world, and if a person saves one life, it is as if they’ve saved an entire world.\n",
      "\n",
      "\n",
      "\tWhat an extraordinary honor to be with you as we honor four Righteous individuals whose courage is measured in the lives they saved -- one child, one refugee, one comrade at a time -- and who, in so doing, helped save our world.\n",
      "\n",
      "\n",
      "\tI deliver a lot of speeches.  Very rarely am I so humbled by the eloquence that has preceded me -- not just in words, but in the acts that we commemorate today.\n",
      "\n",
      "\n",
      "\tTo my dear friend, Steven Spielberg, thanks for your moving and generous words.  You spoke of the importance of finding your voice and using it for good, and I know that your work -- whether a masterpiece like Schindler’s List or the stories that you have so persistently preserved through the Shoah Foundation -- is deeply personal.  Steven once said that the story of the Shoah is the story that he was born to tell, rooted in those childhood memories that he just gave you a taste of -- the relatives lost, the stories you heard from your family.  And, Steven, the whole world is grateful that you found your voice, and for the good that you’ve done with that voice.  It will endure for generations.  And so, on behalf of all of us, we are grateful.\n",
      "\n",
      "\n",
      "\tTo Ambassador and Mrs. Dermer, to Nina Totenberg, our friends from the Israeli Embassy and Yad Vashem -- thank you so much for hosting us today. \n",
      "\n",
      "\n",
      "\tLet me just add tonight that our thoughts are also with former Israeli President Shimon Peres.  I had the opportunity to speak with Shimon earlier this week.  I thanked him for his friendship, which has always meant so much to me, personally.  And I thanked him, once again, for the shining example of his leadership.  With his extraordinary life -- in the Haganah,\n"
     ]
    }
   ],
   "source": [
    "# The White House\n",
    "url = \"https://www.whitehouse.gov/the-press-office/2016/01/27/remarks-president-righteous-among-nations-award-ceremony\"\n",
    "text = get_website_text(url, 0, 1)\n",
    "\n",
    "print(text[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Embassy', 'of', 'Israel', 'Washington', ',', 'D.C.', '6:54', 'P.M.', 'EST', 'THE', 'PRESIDENT', ':', 'Thank', 'you', '.', 'Thank', 'you', '.', '(', 'Applause', '.', ')', 'Good', 'evening', '.', 'Erev', 'Tov', '.', 'The', 'Talmud', 'teaches', 'that', 'if', 'a', 'person', 'destroys', 'one', 'life', ',', 'it', 'is', 'as', 'if', 'they', '’', 've', 'destroyed', 'an', 'entire', 'world', ',', 'and', 'if', 'a', 'person', 'saves', 'one', 'life', ',', 'it', 'is', 'as', 'if', 'they', '’', 've', 'saved', 'an', 'entire', 'world', '.', 'What', 'an', 'extraordinary', 'honor', 'to', 'be', 'with', 'you', 'as', 'we', 'honor', 'four', 'Righteous', 'individuals', 'whose', 'courage', 'is', 'measured', 'in', 'the', 'lives', 'they', 'saved', '--', 'one', 'child', ',', 'one', 'refugee', ',', 'one', 'comrade', 'at', 'a', 'time', '--', 'and', 'who', ',', 'in', 'so', 'doing', ',', 'helped', 'save', 'our', 'world', '.', 'I', 'deliver', 'a', 'lot', 'of', 'speeches', '.', 'Very', 'rarely', 'am', 'I', 'so', 'humbled', 'by', 'the', 'eloquence', 'that', 'has', 'preceded', 'me', '--', 'not', 'just', 'in', 'words', ',', 'but', 'in', 'the', 'acts', 'that', 'we', 'commemorate', 'today', '.', 'To', 'my', 'dear', 'friend', ',', 'Steven', 'Spielberg', ',', 'thanks', 'for', 'your', 'moving', 'and', 'generous', 'words', '.', 'You', 'spoke', 'of', 'the', 'importance', 'of', 'finding', 'your', 'voice', 'and', 'using', 'it', 'for', 'good', ',', 'and', 'I', 'know', 'that', 'your', 'work', '--', 'whether', 'a', 'masterpiece', 'like', 'Schindler', '’', 's', 'List', 'or', 'the', 'stories', 'that', 'you', 'have', 'so', 'persistently', 'preserved', 'through', 'the', 'Shoah', 'Foundation', '--', 'is', 'deeply', 'personal', '.', 'Steven', 'once', 'said', 'that', 'the', 'story', 'of', 'the', 'Shoah', 'is', 'the', 'story', 'that', 'he', 'was', 'born', 'to', 'tell', ',', 'rooted', 'in', 'those', 'childhood', 'memories', 'that', 'he', 'just', 'gave', 'you', 'a', 'taste', 'of', '--', 'the', 'relatives', 'lost', ',', 'the', 'stories', 'you', 'heard', 'from', 'your', 'family', '.', 'And', ',', 'Steven', ',', 'the', 'whole', 'world', 'is', 'grateful', 'that', 'you', 'found', 'your', 'voice', ',', 'and', 'for', 'the', 'good', 'that', 'you', '’', 've', 'done', 'with', 'that', 'voice', '.', 'It', 'will', 'endure', 'for', 'generations', '.', 'And', 'so', ',', 'on', 'behalf', 'of', 'all', 'of', 'us', ',', 'we', 'are', 'grateful', '.', 'To', 'Ambassador', 'and', 'Mrs.', 'Dermer', ',', 'to', 'Nina', 'Totenberg', ',', 'our', 'friends', 'from', 'the', 'Israeli', 'Embassy', 'and', 'Yad', 'Vashem', '--', 'thank', 'you', 'so', 'much', 'for', 'hosting', 'us', 'today', '.', 'Let', 'me', 'just', 'add', 'tonight', 'that', 'our', 'thoughts', 'are', 'also', 'with', 'former', 'Israeli', 'President', 'Shimon', 'Peres', '.', 'I', 'had', 'the', 'opportunity', 'to', 'speak', 'with', 'Shimon', 'earlier', 'this', 'week', '.', 'I', 'thanked', 'him', 'for', 'his', 'friendship', ',', 'which', 'has', 'always', 'meant', 'so', 'much', 'to', 'me', ',', 'personally', '.', 'And', 'I', 'thanked', 'him', ',', 'once', 'again', ',', 'for', 'the', 'shining', 'example', 'of', 'his', 'leadership', '.', 'With', 'his', 'extraordinary', 'life', '--', 'in', 'the', 'Haganah', ',', 'and', 'as', 'a', 'founding', 'father', 'of', 'the', 'State', 'of', 'Israel', ',', 'a', 'statesman', 'who', 'has', 'never', 'given', 'up', 'on', 'peace', ',', 'an', 'embodiment', 'of', 'the', 'great', 'alliance', 'between', 'our', 'two', 'nations', '--', 'Shimon', 'inspires', 'us', 'all', '.', 'And', 'this', 'evening', 'we', 'speak', 'for', 'all', 'of', 'us', '--', 'Israelis', ',', 'Americans', ',', 'people', 'around', 'the', 'world', '--', 'in', 'wishing', 'him', 'a', 'full', 'and', 'speedy', 'recovery', '.', 'I', 'also', 'want', 'to', 'just', 'note', 'the', 'presence', 'of', 'two', 'of', 'our', 'outstanding', 'senators', 'from', 'the', 'great', 'state', 'of', 'Tennessee', '.', 'I', 'know', 'that', 'it', '’', 's', 'rare', 'where', 'you', 'have', 'such', 'a', 'extraordinary', 'native', 'of', 'the', 'state', 'being', 'honored', 'in', 'this', 'way', ',', 'but', 'I', 'think', 'it', '’', 's', 'also', 'worth', 'noting', 'that', 'this', 'represents', 'the', 'bipartisan', 'and', 'steadfast', 'support', 'of', 'members', 'of', 'Congress', 'for', 'the', 'security', 'and', 'prosperity', 'of', 'the', 'state', 'of', 'Israel', '.', 'And', 'they', 'act', 'on', 'that', 'every', 'single', 'day', '.', 'To', 'the', 'survivors', ',', 'families', 'of', 'the', 'Righteous', 'and', 'those', 'they', 'saved', ',', 'to', 'all', 'the', 'distinguished', 'guests', ':', 'We', 'gather', 'to', 'honor', 'the', 'newest', 'of', 'the', 'Righteous', 'Among', 'the', 'Nations', 'and', 'make', 'real', 'the', 'call', 'to', '“', 'never', 'forget', ',', '”', 'not', 'just', 'on', 'this', 'day', 'of', 'remembrance', ',', 'but', 'for', 'all', 'days', 'and', 'for', 'all', 'time', '.', 'And', 'at', 'moments', 'like', 'this', ',', 'as', 'I', 'listened', 'to', 'the', 'extraordinary', 'stories', 'of', 'the', 'four', 'that', 'we', 'honor', ',', 'memories', 'come', 'rushing', 'back', 'of', 'the', 'times', 'that', 'I', '’', 've', 'encountered', 'the', 'history', 'and', 'the', 'horror', 'of', 'the', 'Shoah', '--', 'growing', 'up', ',', 'hearing', 'the', 'stories', 'of', 'my', 'great', 'uncle', 'who', 'helped', 'liberate', 'Ohrdruf', ',', 'part', 'of', 'Buchenwald', ',', 'and', 'who', 'returned', 'home', 'so', 'shaken', 'by', 'the', 'suffering', 'that', 'he', 'had', 'seen', 'that', 'my', 'grandmother', 'would', 'tell', 'me', 'he', 'did', 'not', 'speak', 'to', 'anyone', 'for', 'six', 'months', ',', 'just', 'went', 'up', 'in', 'his', 'attic', ',', 'couldn', '’', 't', 'fully', 'absorb', 'the', 'horror', 'that', 'he', 'had', 'witnessed', '.', 'Then', 'having', 'the', 'opportunity', 'to', 'go', 'to', 'Buchenwald', 'myself', 'with', 'my', 'dear', 'friend', ',', 'Elie', 'Wiesel', ',', 'and', 'seeing', 'the', 'ovens', ',', 'the', 'Little', 'Camp', 'where', 'he', 'was', 'held', 'as', 'a', 'boy', '.', 'Standing', 'with', 'survivors', 'in', 'the', 'Old', 'Warsaw', 'Ghetto', '.', 'And', 'then', 'the', 'extraordinary', 'honor', 'of', 'walking', 'through', 'Yad', 'Vashem', 'with', 'Rabbi', 'Lau', 'and', 'seeing', 'the', 'faces', 'and', 'hearing', 'the', 'voices', 'of', 'the', 'lost', ',', 'of', 'blessed', 'memory', '.', 'And', 'then', 'taking', 'my', 'own', 'daughters', 'to', 'visit', 'the', 'Holocaust', 'Museum', '--', 'because', 'our', 'children', 'must', 'know', 'this', 'chapter', 'of', 'our', 'history', ',', 'and', 'that', 'we', 'must', 'never', 'repeat', 'it', '.', 'The', 'four', 'lives', 'we', 'honor', 'tonight', 'make', 'a', 'claim', 'on', 'our', 'conscience', ',', 'as', 'well', 'as', 'our', 'moral', 'imagination', '.', 'We', 'hear', 'their', 'stories', ',', 'and', 'we', 'are', 'forced', 'to', 'ask', 'ourselves', ',', 'under', 'the', 'same', 'circumstances', ',', 'how', 'would', 'we', 'act', '?', 'How', 'would', 'we', 'answer', 'God', '’', 's', 'question', ',', 'where', 'are', 'you', '?', 'Would', 'we', 'show', 'the', 'love', 'of', 'Walery', 'and', 'Maryla', 'Zbijewski', '?', 'There', ',', 'in', 'Warsaw', ',', 'they', 'could', 'have', 'been', 'shot', 'for', 'opening', 'their', 'home', 'to', 'a', 'five-year-old', 'girl', '.', 'Yet', 'they', 'cared', 'for', 'her', 'like', 'one', 'of', 'their', 'own', ',', 'gave', 'her', 'safety', 'and', 'shelter', 'and', 'moments', 'of', 'warmth', ',', 'of', 'family', 'and', 'music', '--', 'a', 'shield', 'from', 'the', 'madness', 'outside', 'until', 'her', 'mother', 'could', 'return', '.', 'Would', 'we', 'have', 'the', 'extraordinary', 'compassion', 'of', 'Lois', 'Gunden', '?', 'She', 'wrote', 'that', 'she', 'simply', 'hoped', 'to', '“', 'add', 'just', 'another', 'ray', 'of', 'love', 'to', 'the', 'lives', 'of', 'these', 'youngsters', '”', 'who', 'had', 'already', 'endured', 'so', 'much', '.', 'And', 'by', 'housing', 'and', 'feeding', 'as', 'many']\n"
     ]
    }
   ],
   "source": [
    "raw = get_website_text(url, 0, 1)\n",
    "tokens = word_tokenize(raw)\n",
    "print(tokens[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load word_freq_nltk.py\n",
    "def words(text, k = 10, r = 0, sw = 0):\n",
    "    \"\"\"\n",
    "    This function returns all alphabetic words of a specified length for a given text\n",
    "    \n",
    "    Defaults, k = 10 and r = 0, sw = 0\n",
    "    - k = the length of the word\n",
    "    - r = the evaluation options\n",
    "        It takes values 0 (the default) or 1 or 2.\n",
    "        0. \"equals\" | len(word) == k\n",
    "        1. \"less than\" | len(word) < k\n",
    "        2. \"greater than\" | len(word) > k\n",
    "\n",
    "    - sw = stop words (English)\n",
    "            Stop Words are high-frequency words like\n",
    "            'the', 'to', 'an', 'a', etc. \n",
    "            In this function, sw takes values\n",
    "            0 (the default) or 1\n",
    "            \n",
    "            The function prints an exception statement if other values are entered. \n",
    "    \"\"\"\n",
    "\n",
    "    # not accounting for stopwords\n",
    "    if sw == 0:\n",
    "        # Option to Return Words == K\n",
    "        if r == 0:\n",
    "            ucw = [w.lower() for w in text if w.isalpha() and len(w) == k]\n",
    "            return ucw\n",
    "        # Option to Return the Words < K\n",
    "        elif r == 1:\n",
    "            ucw = [w.lower() for w in text if w.isalpha() and len(w) < k]\n",
    "            return ucw\n",
    "        # Option to Return the Words > K\n",
    "        elif r == 2:\n",
    "            ucw = [w.lower() for w in text if w.isalpha and len(w) > k]\n",
    "            return ucw\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    elif sw == 1:\n",
    "         # Option to Return Words == K\n",
    "        if r == 0:\n",
    "            ucw = [w.lower() for w in text if w.lower() not in stopwords('english')\\\n",
    "                   and w.isalpha() and len(w) == k]\n",
    "            return ucw\n",
    "        # Option to Return the Words < K\n",
    "        elif r == 1:\n",
    "            ucw = [w.lower() for w in text if w.lower() not in stopwords('english')\\\n",
    "                    and w.isalpha() and len(w) < k]\n",
    "            return ucw\n",
    "        # Option to Return the Words > K\n",
    "        elif r == 2:\n",
    "            ucw = [w.lower() for w in text if w.lower() not in stopwords('english')\\\n",
    "                    and w.isalpha and len(w) > k]\n",
    "            return ucw\n",
    "        else: \n",
    "            pass\n",
    "    else:\n",
    "        print(\"Please input valid stopwords options: 0 = no, 1 = yes\")\n",
    "\n",
    "def freq_words(text, k = 10, r = 0, n = 20, sw = 0):\n",
    "    \"\"\"\n",
    "    This function uses the words function to generate a specified frequency distribution\n",
    "    of the most frequent words and related graph.\n",
    "    \n",
    "    You can specify word length, an equality option (to look for words =, <, or >) a given length.\n",
    "\n",
    "    You can specify how many words to return and if you want to exclude stopwords.\n",
    "\n",
    "    Defaults, k = 10, r = 0, n = 20, sw = 0\n",
    "    - k = the length of the word\n",
    "    - r = the evaluation option. \n",
    "        It takes values 0 (the default), 1, or 2.\n",
    "        0. \"equals\" | len(word) == k\n",
    "        1. \"less than\" | len(word) < k.\n",
    "        2. \"greater than\" | len(word) > k.\n",
    "    -------------------------------------------------\n",
    "    - n = the number of most common results. \n",
    "        The default value is 20. For example, if you\n",
    "        want to see the top 100 results, input 100.\n",
    "    -------------------------------------------------\n",
    "    - sw = stop words (English)\n",
    "        Stop words are high-frequency words like \n",
    "        (the, to and also, is), among others.\n",
    "        \n",
    "        In this function, sw takes values\n",
    "        0 (the default) or 1. \n",
    "\n",
    "        The function prints an exception \n",
    "        statement if other values are entered.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Generate the Frequency Distribution for specified text, k, and r.\n",
    "    fdist = FreqDist(words(text, k, r, sw))\n",
    "\n",
    "    # Clean up the Title of the Text\n",
    "    clean_title0 = str(text).replace(\"<Text: \", \"\").replace(\">\", \"\").replace('[', '').replace(']', '')\n",
    "    clean_title1 = clean_title0.replace(\"'\", '').replace('\"', '').replace(',', '')[0:10]+\"...\"\n",
    "    try:\n",
    "        c2 = clean_title1.split(\" by \")[0].title()\n",
    "    except:\n",
    "        c2 = clean_title0.title()\n",
    "\n",
    "    # Creating Possible Titles\n",
    "    figtitle1 = \"Most Frequent \" + str(k) + \" - Letter Words in \" + c2\n",
    "    figtitle2 = \"Most Frequent Words Less Than \" + str(k) + \"-Letters in \" + c2\n",
    "    figtitle3 = \"Most Frequent Words Greater Than \"+str(k)+\"-Letters in \"+c2\n",
    "    figtitle4 = \"Most Frequent Words of Any Length \"+c2\n",
    "    figelse = \"Most Frequent Words in \"+c2\n",
    "    \n",
    "    # Setting the Title based on Inputs\n",
    "    if r == 0:\n",
    "        figtitle = figtitle1\n",
    "    elif r == 1:\n",
    "        figtitle = figtitle2\n",
    "    elif r == 2 and k != 0:\n",
    "        figtitle = figtitle3\n",
    "    elif r == 2 and k == 0:\n",
    "        figtitle = figtitle4\n",
    "    else:\n",
    "        print (\"else\")\n",
    "        figtitle = figelse\n",
    "    \n",
    "    #Print Plot and Most Common Words\n",
    "    fdist.plot(n, title=figtitle, cumulative=True)\n",
    "    print (figtitle+\":\", '\\n', fdist.most_common(n))\n",
    "    if sw == 1:\n",
    "        print (\"*NOTE: Excluding English Stopwords\")\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LazyCorpusLoader' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lx/pk8l0jrj46s2mmm8c8m38p1m0000gn/T/ipykernel_23629/2836712632.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get top 30 words > 7 letters in President obama's Embassy Speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfreq_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/lx/pk8l0jrj46s2mmm8c8m38p1m0000gn/T/ipykernel_23629/2054970525.py\u001b[0m in \u001b[0;36mfreq_words\u001b[0;34m(text, k, r, n, sw)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Generate the Frequency Distribution for specified text, k, and r.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0mfdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Clean up the Title of the Text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lx/pk8l0jrj46s2mmm8c8m38p1m0000gn/T/ipykernel_23629/2054970525.py\u001b[0m in \u001b[0;36mwords\u001b[0;34m(text, k, r, sw)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Option to Return the Words > K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             ucw = [w.lower() for w in text if w.lower() not in stopwords('english')\\\n\u001b[0m\u001b[1;32m     54\u001b[0m                     and w.isalpha and len(w) > k]\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mucw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/lx/pk8l0jrj46s2mmm8c8m38p1m0000gn/T/ipykernel_23629/2054970525.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Option to Return the Words > K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             ucw = [w.lower() for w in text if w.lower() not in stopwords('english')\\\n\u001b[0m\u001b[1;32m     54\u001b[0m                     and w.isalpha and len(w) > k]\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mucw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LazyCorpusLoader' object is not callable"
     ]
    }
   ],
   "source": [
    "# get top 30 words > 7 letters in President obama's Embassy Speech\n",
    "freq_words(tokens, 7, 2, 30, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
