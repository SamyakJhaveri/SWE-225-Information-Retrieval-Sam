{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Reference to : https://medium.com/geekculture/different-ways-to-calculate-cosine-similarity-in-python-ae5bb28c372c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '''Sam loves writing about quantum computing'''\n",
    "text2 = '''Sam loves writing for research publications'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [text1, text2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>computing</th>\n",
       "      <th>for</th>\n",
       "      <th>loves</th>\n",
       "      <th>publications</th>\n",
       "      <th>quantum</th>\n",
       "      <th>research</th>\n",
       "      <th>sam</th>\n",
       "      <th>writing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0.470426</td>\n",
       "      <td>0.470426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334712</td>\n",
       "      <td>0.334712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470426</td>\n",
       "      <td>0.334712</td>\n",
       "      <td>0.470426</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.470426</td>\n",
       "      <td>0.334712</td>\n",
       "      <td>0.334712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         about  computing       for     loves  publications   quantum  \\\n",
       "doc1  0.470426   0.470426  0.000000  0.334712      0.000000  0.470426   \n",
       "doc2  0.000000   0.000000  0.470426  0.334712      0.470426  0.000000   \n",
       "\n",
       "      research       sam   writing  \n",
       "doc1  0.000000  0.334712  0.334712  \n",
       "doc2  0.470426  0.334712  0.334712  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "transfrom = vectorizer.fit_transform(corpus)\n",
    "pd.DataFrame(transfrom.toarray(), columns = vectorizer.get_feature_names_out(), index = ['doc1', 'doc2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.33609693]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity(transfrom[0], transfrom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 2\n",
    "With Reference to: https://www.geeksforgeeks.org/python-measure-similarity-between-two-sentences-using-cosine-similarity/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = \"I love comedy movies\"\n",
    "Y = \"Knocked Up is a comedy movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "X_list = word_tokenize(X)\n",
    "Y_list = word_tokenize(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "l1 = []\n",
    "l2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_set = {w for w in X_list if not w in sw}\n",
    "Y_set = {w for w in Y_list if not w in sw}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# form  set containing keywords of both strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.125\n"
     ]
    }
   ],
   "source": [
    "rvector = X_set.union(Y_set)\n",
    "for w in rvector:\n",
    "    if w in X_set: \n",
    "        l1.append(1) # Create a vector\n",
    "    else: \n",
    "        l1.append(0)\n",
    "    \n",
    "    if w in Y_set:\n",
    "        l2.append(1)\n",
    "    else:\n",
    "        l2.append(0)\n",
    "c = 0\n",
    "\n",
    "# cosine formula\n",
    "for i in range(len(rvector)):\n",
    "    c += l1[i] * l2[i]\n",
    "cosine = c / float((sum(l1) * sum(l2) ** 0.5))\n",
    "print(\"Cosine Similarity:\", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 3 \n",
    "With reference to: https://sites.temple.edu/tudsc/2017/03/30/measuring-similarity-between-texts-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining coprus\n",
    "d1 = \"plot: two teen couples go to a church party, drink and then drive.\"\n",
    "d2 = \"films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \"\n",
    "d3 = \"every now and then a movie comes along from a suspect studio , with every indication that it will be a stinker , and to everybody's surprise ( perhaps even the studio ) the film becomes a critical darling . \"\n",
    "d4 = \"damn that y2k bug . \"\n",
    "documents = [d1, d2, d3, d4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize by stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import nltk, string, numpy\\nnltk.download('punkt')\\nstemmer = nltk.stem.porter.PorterStemmer()\\n\\ndef StemTokens(tokens):\\n    return [stemmer.stem(token) for token in tokens]\\n\\nremove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\\ndef StemNormalize(text):\\n    return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import nltk, string, numpy\n",
    "nltk.download('punkt')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def StemTokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "def StemNormalize(text):\n",
    "    return StemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize by lemmentization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/samyakjhaveri/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, string, numpy\n",
    "nltk.download('wordnet')\n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn text into vectors of term frquency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x41 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "LemVectorizer = CountVectorizer(tokenizer = LemNormalize, stop_words='english')\n",
    "LemVectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized (after lemmetization) text in the four documents are tokenized and each term is indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plot': 27, 'teen': 37, 'couple': 9, 'church': 6, 'party': 25, 'drink': 14, 'drive': 15, 'film': 17, 'adapted': 0, 'comic': 8, 'book': 3, 'plenty': 26, 'success': 32, 'theyre': 38, 'superheroes': 33, 'batman': 2, 'superman': 34, 'spawn': 29, 'geared': 18, 'kid': 22, 'casper': 5, 'arthouse': 1, 'crowd': 11, 'ghost': 19, 'world': 39, 'really': 28, 'like': 23, 'hell': 20, 'movie': 24, 'come': 7, 'suspect': 36, 'studio': 31, 'indication': 21, 'stinker': 30, 'everybodys': 16, 'surprise': 35, 'critical': 10, 'darling': 13, 'damn': 12, 'y2k': 40, 'bug': 4}\n"
     ]
    }
   ],
   "source": [
    "print(LemVectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0]\n",
      " [1 1 1 2 0 1 0 0 2 0 0 1 0 0 0 0 0 1 1 1 1 0 1 1 0 0 1 0 1 1 0 0 1 1 1 0\n",
      "  0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 1 2 0 0 0 1\n",
      "  1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "tf_matrix = LemVectorizer.transform(documents).toarray()\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 41)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate tf-idf values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.51082562\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073\n",
      " 1.91629073 1.91629073 1.91629073 1.91629073 1.91629073]\n"
     ]
    }
   ],
   "source": [
    "# Get idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfTran = TfidfTransformer(norm=\"l2\")\n",
    "tfidfTran.fit(tf_matrix)\n",
    "print(tfidfTran.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idf for terms that appear in one document:1.916290731874155\n",
      "The idf of terms that appear in two documents:1.5108256237659907\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def idf(n, df):\n",
    "    result = math.log((n + 1.0) / (df + 1.0)) + 1\n",
    "    return result\n",
    "print(\"The idf for terms that appear in one document:\" + str(idf(4, 1)))\n",
    "print(\"The idf of terms that appear in two documents:\" + str(idf(4, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the tf-idf matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.         0.         0.\n",
      "  0.37796447 0.         0.         0.37796447 0.         0.\n",
      "  0.         0.         0.37796447 0.37796447 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.37796447 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.37796447 0.         0.         0.        ]\n",
      " [0.19381304 0.19381304 0.19381304 0.38762607 0.         0.19381304\n",
      "  0.         0.         0.38762607 0.         0.         0.19381304\n",
      "  0.         0.         0.         0.         0.         0.15280442\n",
      "  0.19381304 0.19381304 0.19381304 0.         0.19381304 0.19381304\n",
      "  0.         0.         0.19381304 0.         0.19381304 0.19381304\n",
      "  0.         0.         0.19381304 0.19381304 0.19381304 0.\n",
      "  0.         0.         0.19381304 0.19381304 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.27094807 0.         0.         0.27094807 0.\n",
      "  0.         0.27094807 0.         0.         0.27094807 0.21361857\n",
      "  0.         0.         0.         0.27094807 0.         0.\n",
      "  0.27094807 0.         0.         0.         0.         0.\n",
      "  0.27094807 0.54189613 0.         0.         0.         0.27094807\n",
      "  0.27094807 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.57735027 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.57735027 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix = tfidfTran.transform(tf_matrix)\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the pairwise similarity matrix (n by n):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         0.        ]\n",
      " [0.         1.         0.03264186 0.        ]\n",
      " [0.         0.03264186 1.         0.        ]\n",
      " [0.         0.         0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "cos_similarity_matrix = (tfidf_matrix * tfidf_matrix.T).toarray()\n",
    "print(cos_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use TfidfVectorizer instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "order must be str, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/samyakjhaveri/Desktop/Drive Folder/Winter 2022/SWE 225 Information Retreival /Assignments/Search Engine/trial_cosine_similarity.ipynb Cell 39'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000037?line=3'>4</a>\u001b[0m     tfidf \u001b[39m=\u001b[39m TfidfVec\u001b[39m.\u001b[39mfit_transform(textlist)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000037?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (tfidf \u001b[39m*\u001b[39m tfidf\u001b[39m.\u001b[39mT)\u001b[39m.\u001b[39mtoarray(())\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000037?line=5'>6</a>\u001b[0m cos_similarity(documents)\n",
      "\u001b[1;32m/Users/samyakjhaveri/Desktop/Drive Folder/Winter 2022/SWE 225 Information Retreival /Assignments/Search Engine/trial_cosine_similarity.ipynb Cell 39'\u001b[0m in \u001b[0;36mcos_similarity\u001b[0;34m(textlist)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000037?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcos_similarity\u001b[39m(textlist):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000037?line=3'>4</a>\u001b[0m     tfidf \u001b[39m=\u001b[39m TfidfVec\u001b[39m.\u001b[39mfit_transform(textlist)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000037?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (tfidf \u001b[39m*\u001b[39;49m tfidf\u001b[39m.\u001b[39;49mT)\u001b[39m.\u001b[39;49mtoarray(())\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py:1051\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=1048'>1049</a>\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m order \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=1049'>1050</a>\u001b[0m     order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39m'\u001b[39m\u001b[39mcf\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=1050'>1051</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_toarray_args(order, out)\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=1051'>1052</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mc_contiguous \u001b[39mor\u001b[39;00m out\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mf_contiguous):\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_compressed.py?line=1052'>1053</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOutput array must be C or F contiguous\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py:1288\u001b[0m, in \u001b[0;36mspmatrix._process_toarray_args\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1285'>1286</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1286'>1287</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///opt/homebrew/lib/python3.9/site-packages/scipy/sparse/_base.py?line=1287'>1288</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mzeros(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, dtype\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdtype, order\u001b[39m=\u001b[39;49morder)\n",
      "\u001b[0;31mTypeError\u001b[0m: order must be str, not tuple"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "TfidfVec = TfidfVectorizer(tokenizer = LemNormalize, stop_words='english')\n",
    "def cos_similarity(textlist):\n",
    "    tfidf = TfidfVec.fit_transform(textlist)\n",
    "    return (tfidf * tfidf.T).toarray(())\n",
    "cos_similarity(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 4 Ultimate Guide to Text Similarity using Python\n",
    "With reference to: https://newscatcherapi.com/blog/ultimate-guide-to-text-similarity-with-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x, y):\n",
    "    ''' returns Jacard Similarity between two lists '''\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality / float(union_cardinality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"The bottle is empty\", \"There is nothing in the bottle\"]\n",
    "sentences = [sent.lower().split(\" \") for sent in sentences]\n",
    "jaccard_similarity(sentences[0], sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance a.k.a L2 norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt, pow, expm1\n",
    "def squared_sum(x):\n",
    "    ''' return 3 rounded square rooted value '''\n",
    "\n",
    "    return round(sqrt(sum(a*a for a in x)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y):\n",
    "    ''' return euclidean distance between two lists '''\n",
    "    return sqrt(sum(pow(a-b, 2) for a, b in zip(x, y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/samyakjhaveri/Desktop/Drive Folder/Winter 2022/SWE 225 Information Retreival /Assignments/Search Engine/trial_cosine_similarity.ipynb Cell 47'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000046?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000046?line=1'>2</a>\u001b[0m embeddings \u001b[39m=\u001b[39m [nlp(sentence)\u001b[39m.\u001b[39mvector \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m sentences]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000046?line=3'>4</a>\u001b[0m distance \u001b[39m=\u001b[39m euclidean_distance(embeddings[\u001b[39m0\u001b[39m], embeddings[\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "embeddings = [nlp(sentence).vector for sentence in sentences]\n",
    "\n",
    "distance = euclidean_distance(embeddings[0], embeddings[1])\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embeddings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/samyakjhaveri/Desktop/Drive Folder/Winter 2022/SWE 225 Information Retreival /Assignments/Search Engine/trial_cosine_similarity.ipynb Cell 49'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000047?line=3'>4</a>\u001b[0m     denominator \u001b[39m=\u001b[39m squared_sum(x) \u001b[39m*\u001b[39m squared_sum(y)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000047?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mround\u001b[39m(numerator \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(denominator), \u001b[39m3\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000047?line=5'>6</a>\u001b[0m cos_similarity(embeddings[\u001b[39m0\u001b[39m], embeddings[\u001b[39m1\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "def cos_similarity(x, y):\n",
    "    ''' return cosine similarity between two lists '''\n",
    "    numerator = sum(a*b for a, b in zip(x, y))\n",
    "    denominator = squared_sum(x) * squared_sum(y)\n",
    "    return round(numerator / float(denominator), 3)\n",
    "cos_similarity(embeddings[0], embeddings[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = [\n",
    "#Crypto\n",
    "'Investors unfazed by correction as crypto funds see $154 million inflows',\n",
    "'Bitcoin, Ethereum prices continue descent, but crypto funds see inflows',\n",
    " \n",
    "#Inflation\n",
    "'The surge in euro area inflation during the pandemic: transitory but with upside risks',\n",
    "\"Inflation: why it's temporary and raising interest rates will do more harm than good\",\n",
    " \n",
    "#common\n",
    "'Will Cryptocurrency Protect Against Inflation?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "labels = [headline[:20] for headline in headlines]\n",
    "\n",
    "def create_heatmap(similarity, cmap = \"YlGnBu\"):\n",
    "    df = pd.DataFrame(similarity)\n",
    "    df.columns = labels\n",
    "    df.index = labels\n",
    "    fig, ax = plt.subplots(figsize = (5, 5))\n",
    "    sns.heatmap(df, cmap = cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame constructor not properly called!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/samyakjhaveri/Desktop/Drive Folder/Winter 2022/SWE 225 Information Retreival /Assignments/Search Engine/trial_cosine_similarity.ipynb Cell 52'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000051?line=4'>5</a>\u001b[0m X \u001b[39m=\u001b[39m vectorizer\u001b[39m.\u001b[39mfit_transform(headlines)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000051?line=5'>6</a>\u001b[0m arr \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000051?line=7'>8</a>\u001b[0m create_heatmap(cosine_similarity)\n",
      "\u001b[1;32m/Users/samyakjhaveri/Desktop/Drive Folder/Winter 2022/SWE 225 Information Retreival /Assignments/Search Engine/trial_cosine_similarity.ipynb Cell 51'\u001b[0m in \u001b[0;36mcreate_heatmap\u001b[0;34m(similarity, cmap)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000050?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_heatmap\u001b[39m(similarity, cmap \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYlGnBu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000050?line=4'>5</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(similarity)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000050?line=5'>6</a>\u001b[0m     df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m labels\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/samyakjhaveri/Desktop/Drive%20Folder/Winter%202022/SWE%20225%20Information%20Retreival%20/Assignments/Search%20Engine/trial_cosine_similarity.ipynb#ch0000050?line=6'>7</a>\u001b[0m     df\u001b[39m.\u001b[39mindex \u001b[39m=\u001b[39m labels\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py:756\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py?line=752'>753</a>\u001b[0m \u001b[39m# For data is scalar\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py?line=753'>754</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py?line=754'>755</a>\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py?line=755'>756</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mDataFrame constructor not properly called!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py?line=757'>758</a>\u001b[0m     \u001b[39m# Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py?line=758'>759</a>\u001b[0m     \u001b[39m# expected \"Union[Union[Union[ExtensionArray, ndarray],\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py?line=759'>760</a>\u001b[0m     \u001b[39m# Index, Series], Sequence[Any]]\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/lib/python3.9/site-packages/pandas/core/frame.py?line=760'>761</a>\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(headlines)\n",
    "arr = X.toarray()\n",
    "\n",
    "create_heatmap(cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial 5\n",
    "With Reference to: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
